GPT 是 Generative Pre-trained Transformer 的缩写，中文常见翻译是：

生成式 · 预训练 · 变换器（模型）

简单拆开说就是：

Generative（生成式）：
我不是只查答案，而是根据上下文生成新的文字。

Pre-trained（预训练）：
在正式和你对话前，我已经用大量文本数据学习过语言规律。

Transformer（变换器）：
这是我使用的一种神经网络结构，特别擅长理解上下文和长文本。

Transformer 是让我变“聪明”的关键结构：

它的强项是 👉 注意力机制（Attention）


真正“微调”一个模型（普通人也能）

这一步是真的改权重，但不是我这个模型。

---
用 LLaMA / Mistral / Qwen / Yi 这类开源模型

用你自己的数据做 Fine-tuning / LoRA

RAG

RLHF
